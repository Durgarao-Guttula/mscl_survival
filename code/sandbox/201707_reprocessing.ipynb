{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing of 201707 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bokeh' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2f91df07cd5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmscl_utils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmscl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmscl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_plotting_style\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mbokeh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bokeh' is not defined"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import tqdm as tq\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xmltodict\n",
    "import json\n",
    "\n",
    "import skimage.io\n",
    "import skimage.exposure\n",
    "import skimage.morphology\n",
    "import scipy.ndimage\n",
    "\n",
    "import mscl_utils as mscl\n",
    "colors = mscl.set_plotting_style()\n",
    "bokeh.io.output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to reprocess all data from March - July of 2017 for the MscL project. I rewrote a bunch of functions for proper background subtraction and flatfield illumination that will be important for the final analysis.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to be used "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are all of the functions to be used in this processing. I am trying to keep these up to date with the functions found in the `mscl_utils` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For background subtraction\n",
    "def compute_mean_bg(phase_image, fluo_image, method='isodata', obj_dark=True):\n",
    "    \"\"\"\n",
    "    Computes the mean background fluorescence of the inverted segmentation mask. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    phase_image : 2d-array, int or float. \n",
    "        The phase contrast image used for generating the inverse segmentation mask.\n",
    "        If this image is not a float with pixel values in (0, 1), it will be\n",
    "        renormalized.\n",
    "    fluo_image : 2d-array, int\n",
    "        The fluorescence image used to calculate the mean pixel value. If flatfield\n",
    "        correction is necessary, it should be done before this sending to this function.\n",
    "    method: string, ['otsu', 'yen', 'li', 'isodata'], default 'isodata'\n",
    "        Automated thresholding method to use. Default is 'isodata' method. \n",
    "    obj_dark : bool, default True\n",
    "        If True, objects will be **darker** than the automatically generated\n",
    "        threshold value. If False, objects are deemed to be brighter. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mean_bg: float\n",
    "        The mean background fluorescence of the image. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure that the image is renormalized. \n",
    "    if (phase_image > 1.0).any():\n",
    "        phase_image = (phase_image - phase_image.min()) /\\\n",
    "                      (phase_image.max() - phase_image.min())\n",
    "    # Perform the background subtraction. \n",
    "    im_blur = skimage.filters.gaussian(phase_image, sigma=50)\n",
    "    im_sub = phase_image - im_blur\n",
    "    \n",
    "    # Determine the method to use. \n",
    "    methods = {'otsu': skimage.filters.threshold_otsu,\n",
    "               'yen': skimage.filters.threshold_yen,\n",
    "               'li': skimage.filters.threshold_li,\n",
    "               'isodata': skimage.filters.threshold_isodata}\n",
    "    \n",
    "    # Determine the threshold value. \n",
    "    thresh_val = methods[method](im_sub)\n",
    "    \n",
    "    # Generate the inverted segmentation mask and dilate. \n",
    "    if obj_dark is True:\n",
    "        im_thresh = im_sub < thresh_val\n",
    "    else:\n",
    "        im_thresh = im_sub > thresh_val\n",
    "        \n",
    "    selem = skimage.morphology.disk(20)\n",
    "    im_dil = skimage.morphology.dilation(im_thresh, selem=selem)\n",
    "    \n",
    "    # Mask onto the fluroescence image and compute the mean background value. \n",
    "    mean_bg = np.mean(fluo_image[im_dil < 1]) \n",
    "    return mean_bg\n",
    "\n",
    "## For flat-field illumination\n",
    "def median_flatfield(image_stack, medfilter=True, selem='default',\n",
    "                    return_profile=False):\n",
    "    \"\"\"\n",
    "    Computes a illumination profile from the median of all images \n",
    "    and corrects each individual image.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image_stack: scikit-image ImageCollection\n",
    "        Series of images to correct. The illumination profile is created\n",
    "        from computing the median filter of all images in this collection.\n",
    "    medfilter: bool, default True\n",
    "        If True, each individiual image will be prefiltered using a median\n",
    "        filter with  a given selem.\n",
    "    selem : string or structure, default 3x3 square\n",
    "        Structural element to use for the median filtering. Default  is \n",
    "        a 3x3 pixel square.\n",
    "    return_profile: bool, default False\n",
    "        If True, the illumination profiled image will be returned.\n",
    "   \n",
    "    Returns\n",
    "    -------\n",
    "    ff_ims : list of 2d-array\n",
    "        Flatfield corrected images.\n",
    "    med_im : 2d-array\n",
    "        Illumination profile produced from the median of all images in\n",
    "        image stack.\n",
    "    \"\"\"\n",
    " \n",
    "    # Determine if the prefiltering should be performed.\n",
    "    if medfilter is True:\n",
    "        \n",
    "        # Define the structural element. \n",
    "        if selem is 'default':\n",
    "            selem = skimage.morphology.square(3)\n",
    "        image_stack = [scipy.ndimage.median_filter(im, footprint=selem) for im in image_stack]\n",
    "    \n",
    "    # Compute the median filtered image.\n",
    "    med_im = np.median(image_stack, axis=0)\n",
    "    \n",
    "    # Perform the correction. \n",
    "    ff_ims = [(i / med_im) * np.mean(med_im) for i in image_stack]\n",
    "    \n",
    "    if return_profile is True:\n",
    "        return [ff_ims, med_im]\n",
    "    else:\n",
    "        return ff_ims\n",
    "    \n",
    "## For segmentation\n",
    "def contour_seg(image, level=0.3, selem='default', perim_bounds=(5, 1E3),\n",
    "                ip_dist=0.160, ecc_bounds=(0.7, 1), area_bounds=(1, 50), \n",
    "                return_conts=False, min_int=0.2):\n",
    "    \"\"\"\n",
    "    Identifies contours around dark objects in a phase contrast image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image: 2d-array\n",
    "        Phase contrast image of interest. \n",
    "    level: float\n",
    "        Level at which to draw contours on black top-hat filtered image.\n",
    "        Default value is 0.3.\n",
    "    selem: 2d-array or string\n",
    "        Structuring element to use for the black top-hat filtering procedure\n",
    "        Default value is a disk with a diameter of 20 pixels.\n",
    "    perim_bounds: length 2 tuple\n",
    "        Lower and upper perimeter bounds of approved objects. This should be\n",
    "        in units of microns. The default values are 5 and 25 microns for the\n",
    "        lower and upper bound, respectively.\n",
    "    ip_dist : float\n",
    "        Interpixel distance of the image in units of microns per pixel. The\n",
    "        default value is 0.160 microns per pixel.\n",
    "    area_bounds : tuple of float\n",
    "        Upper and lower bounds for selected object areas. These should be given in\n",
    "        units of square microns. \n",
    "    ecc_bounds : tuple of float\n",
    "        Bounds for object eccentricity. Default values are between 0.5 and 1.0. \n",
    "    return_conts : bool\n",
    "        If True, the x and y coordinates of the individual contours will be\n",
    "        returned. Default value is False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    im_lab : 2d-array, int\n",
    "        Two dimensional image where each individual object is labeled.\n",
    "\n",
    "    conts : 1d-array\n",
    "        List of contour coordinates. Each entry of this array comes as\n",
    "        an x,y pair of arrays. Has the same length as the number of\n",
    "        contoured objects. This is only returned if `return_conts` is\n",
    "        True.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply the white top-hat filter.\n",
    "    if selem == 'default':\n",
    "        selem = skimage.morphology.disk(20)\n",
    "\n",
    "    # Normalize the image.\n",
    "    image = (image - image.min()) / (image.max() - image.min())\n",
    "\n",
    "    # Blur and background subtract the image.\n",
    "    im_blur = skimage.filters.gaussian(image, sigma=5)\n",
    "    im_sub = image - im_blur\n",
    "\n",
    "    # Apply the black tophat filter.\n",
    "    im_filt = skimage.morphology.black_tophat(im_sub, selem)\n",
    "\n",
    "    # Find the contours and return.\n",
    "    conts = skimage.measure.find_contours(im_filt, level)\n",
    "\n",
    "    # Make an empty image for adding the approved objects.\n",
    "    objs = np.zeros_like(image)\n",
    "\n",
    "    # Loop through each contour.\n",
    "    for _, c in enumerate(conts):\n",
    "        perim = 0\n",
    "        for j in range(len(c) - 1):\n",
    "            # Compute the distance between points.\n",
    "            distance = np.sqrt((c[j+1, 0] - c[j, 0])**2 +\n",
    "                               (c[j+1, 1] - c[j, 1])**2)\n",
    "            perim += distance * ip_dist\n",
    "\n",
    "        # Test if the perimeter is allowed by the user defined bounds.\n",
    "        if (perim > perim_bounds[0]) & (perim < perim_bounds[1]):\n",
    "\n",
    "            # Round the contours.\n",
    "            c_int = np.round(c).astype(int)\n",
    "\n",
    "            # Color the image with the contours and fill.\n",
    "            objs[c_int[:, 0], c_int[:, 1]] = 1.0\n",
    "\n",
    "    # Fill and label the objects.\n",
    "    objs_fill = scipy.ndimage.binary_fill_holes(objs)\n",
    "    objs_fill = skimage.morphology.remove_small_objects(objs_fill)\n",
    "    im_lab = skimage.measure.label(objs_fill)\n",
    "   \n",
    "    # Apply filters.\n",
    "    approved_obj = np.zeros_like(im_lab)\n",
    "    props = skimage.measure.regionprops(im_lab, image)\n",
    "    for prop in props:\n",
    "        area = prop.area * ip_dist**2\n",
    "        ecc = prop.eccentricity\n",
    "        if (area < area_bounds[1]) & (area > area_bounds[0]) &\\\n",
    "            (ecc < ecc_bounds[1]) & (ecc > ecc_bounds[0]) & (prop.mean_intensity < min_int):\n",
    "                approved_obj +=  (im_lab == prop.label) \n",
    "    im_lab = skimage.measure.label(approved_obj)\n",
    "    \n",
    "    \n",
    "    if return_conts is True:\n",
    "        return conts, im_lab\n",
    "    else:\n",
    "        return im_lab\n",
    "    \n",
    "\n",
    "def marker_parse(fname, type_dict={1: False, 2: True}):\n",
    "    \"\"\"\n",
    "    Parses the XML file produced from the CellCounter ImageJ plugin and\n",
    "    packages the marker positions and type into a Pandas DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fname : str\n",
    "        Path to the XML file of interest.\n",
    "    type_dict : dict\n",
    "        Dictionary of types and survival. Default is assigning type 1\n",
    "        as death and type 2 as survival.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : Pandas DataFrame\n",
    "        Data frame containing x and y positions of markers as well as\n",
    "        the type classification.\n",
    "    \"\"\"\n",
    "    with open(fname, 'r') as f:\n",
    "        positions = xmltodict.parse(f.read())\n",
    "\n",
    "    # Extract only the marker data.\n",
    "    markers =  positions['CellCounter_Marker_File']['Marker_Data']['Marker_Type']\n",
    "\n",
    "    # Find the total number of types and loop through them to make data frames.\n",
    "    dfs = []\n",
    "    num_types = len(markers)\n",
    "    for i in range(num_types):\n",
    "        try:\n",
    "            type_marks = markers[i]['Marker']\n",
    "            _df = pd.DataFrame(type_marks)\n",
    "            # Insert a column keeping track of the type\n",
    "            _df.insert(0, 'survival', type_dict[int(markers[i]['Type'])])\n",
    "            dfs.append(_df)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Concatenate the data frames ignorning indexing.\n",
    "    df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "    # Clean up the data frame and return.\n",
    "    df.drop('MarkerZ', 1, inplace=True)\n",
    "    df.columns = ['survival', 'x_pos', 'y_pos']\n",
    "    df['x_pos'] = df['x_pos'].astype(int)\n",
    "    df['y_pos'] = df['y_pos'].astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "def link_markers(markers, seg_mask, fluo_image, ip_dist=0.160,\n",
    "                 return_coords=False, inplace=False, max_dist=5,\n",
    "                 position_labels=('x_pos', 'y_pos')):\n",
    "    \"\"\"\n",
    "    Maps markers from one image to centroids of segmented objects from\n",
    "    another. This assumes a marker belongs to the object with the minimum\n",
    "    marker-centroid distance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    markers : Pandas DataFrame\n",
    "        DataFrame containing the x and y positions of the markers.\n",
    "    seg_mask : 2d-array, int\n",
    "        Labeled segmentation mask. The coordinates of the object centroids\n",
    "        will be calculated from this image.\n",
    "    fluo_image : 2d-array, float or int\n",
    "        The fluorescence image used to extract intensities. If None, no\n",
    "        intensity information will be returned. These intensity values\n",
    "        will be returned as an intensity per square physical distance\n",
    "        as given by `ip_dist`.\n",
    "    ip_dist :  float\n",
    "        Interpixel distance for the image. Default value is 0.160 microns\n",
    "        per pixel.\n",
    "    return_coords : bool\n",
    "        If True, the paired coordinates will be returned as a tuple. It\n",
    "        will have the form ((mark_x, mark_y), (cent_x, cent_y)). Default\n",
    "        value is False.\n",
    "    inplace : bool\n",
    "        If True, the markers DataFrame will be updated in place with the\n",
    "        paired mask label and intensity if the fluorescence image is given.\n",
    "    max_dist : float\n",
    "        Maximum distance to keep. Default Value is 5 microns. \n",
    "    position_labels :  tuple of str\n",
    "        Labels of position markers in the markers DataFrame in the order\n",
    "        of x position and y position. Default is `x_pos` and `y_pos`.\n",
    "\n",
    "   Returns\n",
    "   -------\n",
    "   df : Pandas DataFrame\n",
    "       DataFrame containing survival type, marker positions, mask label,\n",
    "       area, and intensity if provided. Note this is not returned if\n",
    "       `in_place = True`.\n",
    "   coords : list of tuple\n",
    "       A list of tuples containing the marker x,y positions and the\n",
    "       coordinates of the associated segmentation centroid. This\n",
    "       is only returned if `return_coords`==True.\n",
    "\n",
    "    \"\"\"\n",
    "    # Compute the properties from the segmentation mask.\n",
    "    props = skimage.measure.regionprops(seg_mask, fluo_image)\n",
    "    area, intensity, labels, centroids = [], [], [], []\n",
    "    for prop in props:\n",
    "        area.append(prop.area * ip_dist**2)\n",
    "        intensity.append(prop.mean_intensity / ip_dist**2)\n",
    "        labels.append(prop.label)\n",
    "        centroids.append(prop.centroid)\n",
    "\n",
    "    # Set up a list to store the coordinates and duplicate the df.\n",
    "    coords = []\n",
    "    if type(markers) == str:\n",
    "        df= pd.DataFrame([intensity, area]).T\n",
    "        df.columns=['intensity','area']\n",
    "        df.insert(np.shape(df)[1], 'dist', 0)\n",
    "        df.insert(0, 'label_cent_y', 0)\n",
    "        df.insert(0, 'label_cent_x', 0)\n",
    "        df.insert(0, 'mask_label', labels )\n",
    "        df.insert(0, 'y_pos', 0)\n",
    "        df.insert(0, 'x_pos', 0)\n",
    "        df.insert(0, 'survival', False)\n",
    "        return df \n",
    "        \n",
    "    else:\n",
    "        df = markers.copy(deep=True)\n",
    "\n",
    "        \n",
    "        # Compute the minimum distances.\n",
    "        for i in range(len(markers)):\n",
    "            distances = []\n",
    "            x = markers.iloc[i][position_labels[0]]\n",
    "            y = markers.iloc[i][position_labels[1]]\n",
    "\n",
    "            # Loop through each centroid and find the minimum distance.\n",
    "            for c in centroids:\n",
    "                dist = np.sqrt((x - c[1])**2 + (y - c[0])**2)\n",
    "                distances.append(dist)\n",
    "            if len(distances) == 0:\n",
    "                df.set_value(i, 'dist', 1E6)\n",
    "                pass\n",
    "            else:\n",
    "                # Find the index with the minimum distance.\n",
    "                min_ind = np.argmin(distances)\n",
    "                coords.append(((x, y),\n",
    "                               (centroids[min_ind][1], centroids[min_ind][0])))\n",
    "\n",
    "                # Determine if a new DataFrame should be made or not.\n",
    "                # There should be a better way to do this -- will spruce up later.\n",
    "                if inplace is False:\n",
    "                    # Update the data frame.\n",
    "                    df.set_value(i, 'mask_label', labels[min_ind])\n",
    "                    df.set_value(i, 'label_cent_x', centroids[min_ind][1])\n",
    "                    df.set_value(i, 'label_cent_y', centroids[min_ind][0])\n",
    "                    df.set_value(i, 'intensity', intensity[min_ind])\n",
    "                    df.set_value(i, 'area', area[min_ind])\n",
    "                    df.set_value(i, 'dist', distances[min_ind])\n",
    "                else:\n",
    "                    markers.set_value(i, 'mask_label', labels[min_ind])\n",
    "                    markers.set_value(i, 'label_cent_x', centroids[min_ind][1])\n",
    "                    markers.set_value(i, 'label_cent_y', centroids[min_ind][0])\n",
    "                    markers.set_value(i, 'intensity', intensity[min_ind])\n",
    "                    markers.set_value(i, 'area', area[min_ind])\n",
    "\n",
    "        # Apply the distance filter. \n",
    "        if inplace is False:\n",
    "            df = df[df['dist'] <= (max_dist / ip_dist)]\n",
    "            if return_coords is True:\n",
    "                return df, coords\n",
    "            else:\n",
    "                return df\n",
    "\n",
    "\n",
    "def scrape_metadata(fname, channels=('Brightfield', 'GFP'), return_date=True):\n",
    "    \"\"\"\n",
    "    Takes an image metadata file and returns the datea nd GFP exposure time.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fname : str\n",
    "        Pat of the metadata file to parse.\n",
    "    channel : tuple of str\n",
    "        The channels from which to scrape the exposure time. A single channel\n",
    "        name can be given. Default is ('Brightfield', 'GFP').\n",
    "    return_date : bool\n",
    "        If True, the date of the acquisition will also be returned.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    exposure: dict or float\n",
    "        The exposure time of the desired channel. If multiple channels are\n",
    "        given, this will be a tuple of the exposure times. If return_date is\n",
    "        True, the date will also be in this dictionary.\n",
    "   \"\"\"\n",
    "\n",
    "    # Open the metadata file.\n",
    "    with open(fname, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    \n",
    "\n",
    "    # Get a list of the keys in the metadata file.\n",
    "    keys = metadata.keys()\n",
    "\n",
    "    # Determine if a single channel or multiple channel exposures are desired.\n",
    "    if (type(channels) != tuple) & (type(channels) != str):\n",
    "        raise TypeError('desired channels must be a tuple or a string.')\n",
    "    else:\n",
    "        if type(channels) == str:\n",
    "            num_channels = 1\n",
    "            channels = (channels)\n",
    "            exposure = None\n",
    "        else:\n",
    "            num_channels = len(channels)\n",
    "            exposure = []\n",
    "\n",
    "    # Loop through each desired channel and scrape the exposure.\n",
    "    for i in range(num_channels):\n",
    "        for k in keys:\n",
    "            try:\n",
    "                chan = metadata[k]['Channel']\n",
    "                if chan.lower() == channels[i].lower():\n",
    "                    _exposure = metadata[k]['Exposure-ms']\n",
    "                if num_channels == 1:\n",
    "                    exposure = _exposure\n",
    "                else:\n",
    "                    if i == 0:\n",
    "                        exposure = {channels[i] + '_exp_ms': _exposure}\n",
    "                    else:\n",
    "                        exposure[channels[i] + '_exp_ms'] = _exposure\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    if return_date is True:\n",
    "        # Get the date from the Summary field.\n",
    "        date = metadata['Summary']['Date'].split('-')\n",
    "        date = ''.join(date)\n",
    "        exposure['date'] = date\n",
    "    return exposure\n",
    "\n",
    "\n",
    "def save_seg(fname, image, mask, fill_contours=True, ip_dist=0.160,\n",
    "             bar_length=10, title=None, colormap='hls'):\n",
    "    \"\"\"\n",
    "    Saves a merge of a segmentation mask and the original image for a\n",
    "    sanity check.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fname : str\n",
    "        The file will be saved with this path.\n",
    "    image : 2d-array, float\n",
    "        The original image on which the segmentation mask will be overlaid.\n",
    "    mask : 2d-array, bool\n",
    "        Boolean segmentation mask of the original image.\n",
    "    contours: bool\n",
    "        If True, contours of segmented objects will be filled.\n",
    "    ip_dist : float\n",
    "        Interpixel distance for the image. This is used for computing the\n",
    "        scalebar length.  This should be in units of microns. Default\n",
    "        value is 0.160 microns per pixel.\n",
    "    bar_length : int\n",
    "        The length of the desired scalebar in units of microns.\n",
    "    title : str, optional\n",
    "        Title for the image.\n",
    "    colormap : str\n",
    "        Colormap for labeling the objects. Default is the high-contrast\n",
    "        'hls'. This can take any standard colormap string.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    fig : Matplotlib Figure object\n",
    "        Figure containing the axis of the plotted image.\n",
    "    \"\"\"\n",
    "\n",
    "    # Make copies of the image and mask.\n",
    "    image_copy = np.copy(image)\n",
    "    mask_copy = np.copy(mask)\n",
    "\n",
    "    # Burn the scalebar into the upper-left hand  of th image.\n",
    "    num_pix = int(bar_length / ip_dist)\n",
    "    image = (image_copy - image_copy.min()) /\\\n",
    "            (image_copy.max() - image_copy.min())\n",
    "    image[10:20, 10:10 + num_pix] = 1.0\n",
    "\n",
    "    # Make sure the mask is a boolean image.\n",
    "    if type(mask) != bool:\n",
    "        mask = mask_copy > 0\n",
    "\n",
    "    # Find the contours of the mask.\n",
    "    conts = skimage.measure.find_contours(mask, 0)\n",
    "\n",
    "    # Plot the image and generate the contours.\n",
    "    with sns.axes_style('white'):\n",
    "        fig = plt.figure()\n",
    "        plt.imshow(image, cmap=plt.cm.Greys_r)\n",
    "\n",
    "        # Plot all of the contours\n",
    "        colors = sns.color_palette(colormap, n_colors=len(conts))\n",
    "        for i, c in enumerate(conts):\n",
    "            plt.plot(c[:, 1], c[:, 0], color=colors[i], lw=0.75)\n",
    "            if fill_contours is True:\n",
    "                plt.fill(c[:, 1], c[:, 0], color=colors[i], alpha=0.5)\n",
    "\n",
    "        # Remove the axes.\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "\n",
    "        # Add title if provided.\n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "\n",
    "        # Tighten up and save the image.\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(fname, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def show_connections(fname, image, data, title=None, bar_length=10,\n",
    "                     ip_dist=0.16):\n",
    "    \"\"\"\n",
    "    Saves the original phase contrast image with the segmented\n",
    "    centroids and the manually recorded markers linked by lines.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fname : str\n",
    "        Filename to save the image wish shown connections between\n",
    "        segmented object centroids and the markers.\n",
    "    image : 2d-array\n",
    "        Original phase contrast image over which the points will\n",
    "        be drawn\n",
    "    data : Pandas DataFrame\n",
    "        DataFrame containing the marker x and y positions and the\n",
    "        centroid x and y positions.\n",
    "    title : str\n",
    "        Title to be applied to the image. If not specified, none will\n",
    "        be included.\n",
    "    bar_length : int\n",
    "        Length of the scalebar in units of microns. Default value\n",
    "        is 10.\n",
    "    ip_dist : float\n",
    "        Interpixel distance of the image. This should be in units of\n",
    "        microns per pixel. Default value is 0.16 microns per pixel.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fig : Matplotlib Figure Canvas\n",
    "        Figure canvas of the plot.\n",
    "    \"\"\"\n",
    "    # Add the scale bar to the image.\n",
    "    if image.max() > 1:\n",
    "        image = (image - image.min()) / (image.max() - image.min())\n",
    "    num_pix = int(bar_length / ip_dist)\n",
    "    image_copy = np.copy(image)\n",
    "    image_copy[10:20, 10:10 + num_pix] = 1.0\n",
    "\n",
    "    # Define the colors for survivors and corpses.\n",
    "    colors = {False: '#D56C55', True: '#08AADE'}\n",
    "\n",
    "    # Group the DataFrame by survival.\n",
    "    grouped = pd.groupby(data, 'survival')\n",
    "\n",
    "    # Show the image\n",
    "    with sns.axes_style('white'):\n",
    "        fig = plt.figure()\n",
    "        plt.imshow(image_copy, cmap=plt.cm.Greys_r)\n",
    "        plt.plot([], [], '-o', ms=3, lw=1, color=colors[True],\n",
    "                 label='survivor')\n",
    "        plt.plot([], [], '-o', ms=3, lw=1, color=colors[False], label='goner')\n",
    "        plt.legend(loc='lower left')\n",
    "        for g, d in grouped:\n",
    "            for i in range(len(d)):\n",
    "                # Parse the positions\n",
    "                m_x = d.iloc[i]['x_pos']\n",
    "                m_y = d.iloc[i]['y_pos']\n",
    "                c_x = d.iloc[i]['label_cent_x']\n",
    "                c_y = d.iloc[i]['label_cent_y']\n",
    "                # Plot the connections.\n",
    "                plt.plot((m_x, c_x), (m_y, c_y), '-', ms=3, lw=1,\n",
    "                         color=colors[g])\n",
    "                plt.plot(m_x, m_y, 'o', ms=3, lw=1, color=colors[g])\n",
    "                plt.plot(c_x, c_y, 'o', ms=3, markerfacecolor='w',\n",
    "                         markeredgecolor=colors[g], markeredgewidth=1)\n",
    "\n",
    "        # Format the axes\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "\n",
    "        # Add a title if necessary.\n",
    "        if title is not None:\n",
    "            plt.title(title, fontsize=12)\n",
    "        plt.savefig(fname, bbox_inches='tight')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reprocessing the data\n",
    "\n",
    "Life is pretty miserable with the current data structure. I'm going to reorganize all of the data I have in to a tidier format with standardized names so that I don't have to include all of these separate special cases. The entire directory `/data/reorganized_data/` is set with two subdirectories, `shock_data` and `calibration_data`. The `shock_data` has all of the actual survival measurements (images + markers) with the following folder pattern:\n",
    "\n",
    "* `YYYYMMDD_RBS_PRE/POST_PUMPRATE_SHOCKRATE_SETNUMBER`\n",
    "\n",
    "There is a mix of `.ome.tiff` files and normal `.tiff` files, so I'll need to construct the loop to take care of that. The intensity calibration images have the following pattern:\n",
    "\n",
    "* `YYYYMMDD_RBS_PRE/POST_SETNUMBER`\n",
    "\n",
    "\n",
    "There is one special case I will have to deal with. For the shock data, there is one experiment `20170316_sd6_pre_100ulmin_1.00hz_0000` that only has three marker files. The rest of the fields had no survivors, so HJ didn't spend his time marking them. Unfortunately, I will have to deal with this one as a special case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For total clarity, the pseudocode for the complete processing should be as follows. \n",
    "\n",
    "1. For each `folder` in `shock_data`\n",
    "    1. Determine if sample is composed of `.ome.tiff` or `.tiff`\n",
    "    2. Load all fluorescence images and flatten them.\n",
    "    3. For each `image` in `set_images`:\n",
    "        1. Segment via contouring in phase. \n",
    "        2. Compute mean background fluorescence.\n",
    "        3. Compute background subtraction and compute areal intensity.\n",
    "        4. Load `marker` files and match survival to intensity\n",
    "        5. Compute and save a segmentation mask and connection figure.\n",
    "        5. Store information in `survival_data` pandas DataFrame.\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the pseudocode out of the way, let's actually do it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Executing the processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gchure/anaconda/lib/python3.6/site-packages/skimage/external/tifffile/tifffile.py:1752: UserWarning: ome-xml: not an ome-tiff master file\n",
      "  warnings.warn(\"ome-xml: not an ome-tiff master file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 16sd0 from 20170324 with flow rate 0.02hz.\n",
      "Processing sd1 from 20170421 with flow rate 00.71hz.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gchure/anaconda/lib/python3.6/site-packages/skimage/morphology/misc.py:122: UserWarning: Only one label was provided to `remove_small_objects`. Did you mean to use a boolean array?\n",
      "  warn(\"Only one label was provided to `remove_small_objects`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sd1 from 20170424 with flow rate 1.92hz.\n",
      "Processing 10sd1 from 20170426 with flow rate 0.005hz.\n",
      "Processing 10sd1 from 20170428 with flow rate 01.88hz.\n",
      "Processing sd2 from 20170502 with flow rate 00.02hz.\n",
      "Processing sd2 from 20170503 with flow rate 00.2hz.\n",
      "Processing sd2 from 20170509 with flow rate 02.20hz.\n",
      "Processing sd2 from 20170517 with flow rate 02.17hz.\n",
      "Processing 12sd2 from 20170518 with flow rate 02.00hz.\n",
      "Processing 12sd2 from 20170519 with flow rate 00.50hz.\n",
      "Processing sd1 from 20170525 with flow rate 00.01hz.\n",
      "Processing sd4 from 20170714 with flow rate 00.018hz.\n"
     ]
    }
   ],
   "source": [
    "# Define the data directory. \n",
    "data_dir = 'data/reorganized_data/shock_data/'\n",
    "shock_folders = glob.glob(data_dir + '2017*')\n",
    "\n",
    "dfs = []\n",
    "for i, folder in enumerate(shock_folders):\n",
    "    # Determine the file identifiers.\n",
    "    split_files = folder.split('/')[-1].split('_')\n",
    "    DATE, RBS, _PREPOST , PUMPRATE, SHOCKRATE, _NUM = split_files\n",
    "    \n",
    "    # Determine if the subfolder is composed of `Pos` folders or `.ome.tif'\n",
    "    tif_files = glob.glob(folder + '/*.ome.tif*')\n",
    "    if len(tif_files) > 0: \n",
    "        _ims = skimage.io.ImageCollection(tif_files)\n",
    "        phase_ims = [i[0] for i in _ims]\n",
    "        fluo_ims = [i[1] for i in _ims] \n",
    "        metadata = glob.glob(folder + '/*.txt')\n",
    "    else:\n",
    "        bf_files = glob.glob(folder + '/Pos*/*Brightfield*.tif')\n",
    "        gfp_files = glob.glob(folder + '/Pos*/*GFP*.tif')\n",
    "        phase_ims = skimage.io.ImageCollection(bf_files)\n",
    "        fluo_ims = skimage.io.ImageCollection(gfp_files)\n",
    "        metadata = glob.glob(folder + '/Pos*/*.txt')\n",
    "        \n",
    "    # Load the marker files\n",
    "    markers = glob.glob(folder + '/*.xml')       \n",
    "\n",
    "    # Generate the average illumination profile. \n",
    "    ff_ims = median_flatfield(fluo_ims, medfilter=True)\n",
    "    \n",
    "    # Loop through each phase file and segment. \n",
    "    print('Processing {0} from {1} with flow rate {2}.'.format(RBS, DATE, SHOCKRATE))\n",
    "    for j, ph in enumerate(phase_ims):\n",
    "        seg = contour_seg(ph)\n",
    "        # Parse the marker file. \n",
    "        _markers = marker_parse(markers[j])\n",
    "        _df = link_markers(_markers, seg, ff_ims[j])\n",
    "        \n",
    "        # Scrape the metadata for the exposure. \n",
    "        exposure = scrape_metadata(metadata[j], return_date=False)\n",
    "        \n",
    "        # Compute the mean background intensity. \n",
    "        mean_bg = compute_mean_bg(ph, ff_ims[j])\n",
    "        mean_bg = mean_bg / ip_dist**2\n",
    "        \n",
    "        # Prune the data frame\n",
    "        _df['date'] = DATE\n",
    "        _df['rbs'] = RBS\n",
    "        _df['pump_rate'] = PUMPRATE.split('u')[0]\n",
    "        _df['flow_rate'] = SHOCKRATE.split('hz')[0]\n",
    "        _df['mean_bg'] = mean_bg\n",
    "        _df['exposure_ms'] = exposure['GFP_exp_ms']\n",
    "        \n",
    "        # Append the data frame to the global list. \n",
    "        dfs.append(_df)\n",
    "\n",
    "df = pd.concat(dfs, axis=0)\n",
    "df.drop(['x_pos', 'y_pos', 'mask_label', 'label_cent_x','label_cent_y', 'dist'],\n",
    "       axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gchure/anaconda/lib/python3.6/site-packages/skimage/external/tifffile/tifffile.py:1752: UserWarning: ome-xml: not an ome-tiff master file\n",
      "  warnings.warn(\"ome-xml: not an ome-tiff master file\")\n"
     ]
    }
   ],
   "source": [
    "# Deal with the special case of sd6\n",
    "data_dir = 'data/reorganized_data/special_case/20170316_sd6_pre_100ulmin_1.00hz_0000/'\n",
    "files = glob.glob(data_dir + '/*ome.tif')\n",
    "_ims = skimage.io.ImageCollection(files)\n",
    "phase_ims = [z[0] for z in _ims]\n",
    "fluo_ims = [z[1] for z in _ims]\n",
    "meta_data = glob.glob(data_dir + '/*metadata.txt')\n",
    "special_dfs = []\n",
    "ip_dist = 0.16\n",
    "\n",
    "# Grab the markers. \n",
    "markers = glob.glob(data_dir + '/*.xml')\n",
    "\n",
    "# split_files = data_dir.split('/')[-2].split('_')\n",
    "DATE, RBS, _PREPOST , PUMPRATE, SHOCKRATE, _NUM = data_dir.split('/')[-2].split('_')\n",
    "\n",
    "# filter the images. \n",
    "ff_ims = median_flatfield(fluo_ims)\n",
    "\n",
    "for i, ph in enumerate(phase_ims):\n",
    "    seg = contour_seg(ph)\n",
    " \n",
    "    # Compute the mean background intensity. \n",
    "    mean_bg = compute_mean_bg(ph, ff_ims[i])\n",
    "    mean_bg = mean_bg / ip_dist**2\n",
    "    \n",
    "    # Scrape the metadata for the exposure. \n",
    "    exposure = scrape_metadata(metadata[i], return_date=False)\n",
    "   \n",
    "    for j, m in enumerate(markers):\n",
    "        m_pos = m.split('/')[-1].split('.')[0]\n",
    "        ph_pos = files[i].split('/')[-1].split('_')[-1].split('.')[0]\n",
    "        if m_pos == ph_pos:\n",
    "            # Parse the marker file. \n",
    "            _markers = marker_parse(markers[j])\n",
    "            _df = link_markers(_markers, seg, ff_ims[i])\n",
    "            _df.drop(['x_pos', 'y_pos', 'mask_label', 'label_cent_x','label_cent_y', 'dist'],\n",
    "               axis=1, inplace=True)\n",
    "            # Append the data frame to the global list. \n",
    "           \n",
    "        else:\n",
    "            props = skimage.measure.regionprops(seg, ff_ims[i])\n",
    "            area = [prop.area * ip_dist**2 for prop in props]\n",
    "            intensity = [prop.mean_intensity / ip_dist**2 for prop in props]\n",
    "            _df = pd.DataFrame([intensity, area]).T\n",
    "            _df.columns = ['intensity', 'area']\n",
    "            _df['survival'] = False \n",
    "        # Prune the data frame\n",
    "        _df['date'] = DATE\n",
    "        _df['rbs'] = RBS\n",
    "        _df['pump_rate'] = PUMPRATE.split('u')[0]\n",
    "        _df['flow_rate'] = SHOCKRATE.split('hz')[0]\n",
    "        _df['mean_bg'] = mean_bg\n",
    "        _df['exposure_ms'] = exposure['GFP_exp_ms']       \n",
    "        \n",
    "        \n",
    "        \n",
    "        special_dfs.append(_df)\n",
    "        \n",
    "special_df = pd.concat(special_dfs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge the two data frames together. \n",
    "data = pd.concat([df, special_df], axis=0, ignore_index=True)\n",
    "\n",
    "# Rescale the intensity and subtract the background. \n",
    "data['intensity'] = data['intensity'] - data['mean_bg']\n",
    "max_exp = np.max(data['exposure_ms'].unique())\n",
    "data['rescaled_intensity'] = data['intensity'] * (max_exp / data['exposure_ms'])\n",
    "\n",
    "# Save the dataframe. \n",
    "data.to_csv('data/20170814_reprocessed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/20170814_reprocessed_data.csv')\n",
    "max_exp = np.max(data['exposure_ms'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reprocessing the intensity calibration data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I should also reprocess the data used to calibrate the intensity and correct any errors in the measurement of some stratins (such as `sd1` and `10sd1`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gchure/anaconda/lib/python3.6/site-packages/skimage/morphology/misc.py:122: UserWarning: Only one label was provided to `remove_small_objects`. Did you mean to use a boolean array?\n",
      "  warn(\"Only one label was provided to `remove_small_objects`. \"\n"
     ]
    }
   ],
   "source": [
    "# Load the files from the intensity calibration samples. \n",
    "data_dir = 'data/reorganized_data/calibration_data/'\n",
    "cal_folders = glob.glob(data_dir + '/2017*')\n",
    "ip_dist = 0.16\n",
    "\n",
    "dfs = []\n",
    "for i, folder in enumerate(cal_folders):\n",
    "    # Determine the file identifiers.\n",
    "    split_files = folder.split('/')[-1].split('_')\n",
    "    DATE, RBS, _PREPOST ,_NUM = split_files\n",
    "    \n",
    "    # Determine if the subfolder is composed of `Pos` folders or `.ome.tif'\n",
    "    tif_files = glob.glob(folder + '/*.ome.tif*')\n",
    "    if len(tif_files) > 0: \n",
    "        _ims = skimage.io.ImageCollection(tif_files)\n",
    "        phase_ims = [z[0] for z in _ims]\n",
    "        fluo_ims = [z[1] for z in _ims] \n",
    "        metadata = glob.glob(folder + '/*.txt')\n",
    "    else:\n",
    "        bf_files = glob.glob(folder + '/Pos*/*Brightfield*.tif')\n",
    "        gfp_files = glob.glob(folder + '/Pos*/*GFP*.tif')\n",
    "        phase_ims = skimage.io.ImageCollection(bf_files)\n",
    "        fluo_ims = skimage.io.ImageCollection(gfp_files)\n",
    "        metadata = glob.glob(folder + '/Pos*/*.txt')\n",
    "    \n",
    "    # Generate the average illumination profile. \n",
    "    ff_ims = median_flatfield(fluo_ims, medfilter=True)\n",
    "    \n",
    "    # Segment and extract the important information. \n",
    "    for j, ph in enumerate(phase_ims):\n",
    "        seg = contour_seg(ph)\n",
    "        seg, num_obj = skimage.measure.label(seg > 0, return_num=True)\n",
    "        if num_obj > 0:\n",
    "            mean_bg = compute_mean_bg(ph, ff_ims[j])\n",
    "            mean_bg = mean_bg / ip_dist**2\n",
    "            exposure = scrape_metadata(metadata[j], return_date=False)\n",
    "\n",
    "            # Compute the important properties. \n",
    "            props = skimage.measure.regionprops(seg, ff_ims[j])\n",
    "            intensity = [prop.mean_intensity / ip_dist**2 for prop in props]\n",
    "            area = [prop.area * ip_dist**2 for prop in props]\n",
    "            \n",
    "            # Generate dataframe\n",
    "            _df = pd.DataFrame([intensity, area]).T\n",
    "            _df.columns = ['intensity', 'area']\n",
    "            _df['date'] = DATE\n",
    "            _df['rbs'] = RBS\n",
    "            _df['mean_bg'] = mean_bg\n",
    "            _df['exposure_ms'] = exposure['GFP_exp_ms']    \n",
    "            dfs.append(_df)\n",
    "   \n",
    "\n",
    "# Prune and save the data frame. \n",
    "cal_data = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "cal_data['intensity'] = cal_data['intensity'] - cal_data['mean_bg']\n",
    "cal_data['rescaled_intensity'] = cal_data['intensity'] * (max_exp / cal_data['exposure_ms'])\n",
    "cal_data.to_csv('data/20170814_calibration_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And with that... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can analyze it in another notebook "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
